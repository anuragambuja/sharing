{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Assumptions:\n",
    "    - Every Song belong to an album. A random albumid is generated incase the song is released as single\n",
    "    - An album can contain songs from multiple artists and so each song can belong to multiple genre. Song is being taken as the lowest degree of transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. When logged into the reporting portal, one should be able to see all the music a customer has\n",
    "# purchased with the Company and be able to view the order information (genre, artist, album,\n",
    "# publication date, order date, price, etc.). Please create a database design to track what a client\n",
    "# has purchased. The design should be based on a Big Data warehouse environment and embrace\n",
    "# performance, usability, and minimal storage consumption.\n",
    "\n",
    "Lake Tables:\n",
    "    - Customers\n",
    "        -- customerid - PK\n",
    "        -- customername\n",
    "        -- and other customer related data\n",
    "    - Purchases\n",
    "        -- transactionid - PK\n",
    "        -- customerid -FK\n",
    "        -- songid - FK\n",
    "        -- spendamount\n",
    "        -- transactiondate\n",
    "    - Albums (Dimentional table - One album can have multiple songs, so there will be one to many relationship b/w albumid and songid)\n",
    "        -- albumid  - PK\n",
    "        -- albumname \n",
    "        -- songid - PK\n",
    "        -- songname\n",
    "        -- genere\n",
    "        -- artist\n",
    "        -- publicationdate\n",
    "        -- price \n",
    "\n",
    "Mart Table: Based on transactions: This table will be refreshed weekly/daily depending on the number of transactions done. Apply SCD type 2 on customers to update tge other details\n",
    "    - CustomerPurchases\n",
    "        -- Customerid -PK\n",
    "        -- songspurchasedid - PK\n",
    "        -- spend\n",
    "        -- purchasedate\n",
    "        -- genre\n",
    "        -- artist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. For your design in #1, write a query to find all the music owned by a given client. How do you\n",
    "# tune and optimize your query?\n",
    "\n",
    "select customerid\n",
    "from customerpurchases\n",
    "where customerid = <customerid>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  Write an Impala query to find all the clients, who have purchased a song from artist “Barbeque\n",
    "# Spaceflight” but have never purchased a song from artist “Love Fist”.\n",
    "\n",
    "select customerid \n",
    "from customerpurchases\n",
    "where artist = \"“Barbeque Spaceflight\" \n",
    "and customerid not in (select customerid from customerpurchases where artist = 'Love Fist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write a Hive query that returns the total count and total price of all albums sold, grouped by\n",
    "# genre and month.\n",
    "\n",
    "select genre, month(transactiondate) month, sum(spendamount) netspend\n",
    "from purchases\n",
    "group by genre, month(transactiondate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Write Spark or PySpark code that returns the most recent album or single sold per artist.\n",
    "\n",
    "df = sql(\"\"\"select distinct aritst, album \n",
    "from albums a, (select songpurchaseid\n",
    "from customerpurchses\n",
    "where purchasedate = select (max(purchasedate) from customerpurchses group by artist)b\n",
    "where a.songid = b.songpurchaseid\n",
    "\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    6. Your database is in production and used by analysts. One of the analysts comes by your desk\n",
    "#     and shows you this code. He wants to get the revenue and number of purchase transactions by\n",
    "#     client for 2013 and 2014. How can you help to improve this query?\n",
    "#     select 1 as in2013, AccountId, sum(USD) revenue, count(*) txns\n",
    "#      from prod.PURCHASES\n",
    "#      where DateTime between '2013-01-01' and '2013-12-31'\n",
    "#      group by Accountid\n",
    "#      union all\n",
    "#     select 2 as in2014, AccountId, sum(USD) revenue, count(*) txns\n",
    "#      from prod.PURCHASES\n",
    "#      where DateTime between '2014-01-01' and '2013-12-31'\n",
    "#      group by Accountid\n",
    "\n",
    "select case when year(date_time) = 2013 then in2013 else in2014 end year, sum(USD) revenue, count(1) txns\n",
    "from prod_PURCHASES\n",
    "where DateTIme between  '2013-01-01' and '2014-12-31'\n",
    "group by year(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. When building the ETL pipeline for this warehouse, what validations will you perform to ensure\n",
    "# that the incoming data is clean and accurate? \n",
    "\n",
    "File Validations:\n",
    "    1. Check the file is in utf-8 or converting them to utf is not causing issue\n",
    "    2. Check for special characters in file\n",
    "    3. Check if the received file confers to the agreed DIS with the client eg. column sequence, number of columns\n",
    "\n",
    "Data Validation:\n",
    "    1. Check whether the different keys confers to the business rules. eg. the customerid should be 10 digit number. So, check for if the id is not alphanumeric\n",
    "    2. Check is the amount is not negative\n",
    "    \n",
    "    \n",
    "Check for PII Data:\n",
    "    1. Check whatever data is collected has been agreed with the client\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. How will you ensure that the data can be queried with minimal time and resource consumption?\n",
    "\n",
    "1. The Data modelling should be optimum. We should not unnecessarily normalize the tables as joining tables makes the query run slow\n",
    "2. Use optimal Query - Take care of joins, filters etc\n",
    "3. Create Queue in the cluster for each department so that they can use the cluster efficiently\n",
    "4. Use dynamic spark submission to optimally use the cluster. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
